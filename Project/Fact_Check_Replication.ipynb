{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bGVcdUrA13_v"
      },
      "source": [
        "# Replication of CrossAug: Cross-lingual Data Augmentation for Low-resource Neural Machine Translation\n",
        "\n",
        "## Disclaimer:\n",
        "```javascript\n",
        "This is a work in progress. I will be adding more techniques as I learn them. If you have any suggestions, please feel free to reach out to me.\n",
        "```\n",
        "## Credits:\n",
        "```javascript\n",
        "This work is inspired by the following previous work sources:\n",
        "https://github.com/minwhoo/CrossAug\n",
        "\n",
        "author: Minwoo Lee\n",
        "\n",
        "citation: Minwoo Lee, Seungwon Do, and Sung Ju Hwang. 2020. CrossAug: Cross-lingual Data Augmentation for Low-resource Neural Machine Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). Association for Computational Linguistics, Online, July 5-10, 2020, pages 1-11. https://www.aclweb.org/anthology/2020.acl-main.1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvAPJbPsLcl4"
      },
      "source": [
        "# Downloading the files and setting up configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74z31fOYjZtN",
        "outputId": "32767c1e-0c3b-488c-c9db-cce9da5b188e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
            "100 22.0M  100 22.0M    0     0  7150k      0  0:00:03  0:00:03 --:--:-- 7150k\n",
            "fever+crossaug.train.jsonl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!curl  \"https://drive.google.com/uc?export=download&id=16zumyDSuYV415dBDZiMrPIUA3yUrwJqb\" -L  -o fever+crossaug.train.tar.gz\n",
        "!tar xfv *.gz\n",
        "!mkdir fever_data\n",
        "!mv \"fever+crossaug.train.jsonl\" fever_data/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOH1vY5BCxa9"
      },
      "source": [
        "time took: 632.8004121780396\n",
        "Modify evidence using lexical search-based substitution\n",
        "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16664/16664 [00:06<00:00, 2678.98it/s]\n",
        "time took: 6.220763206481934\n",
        "Augment data\n",
        "Saving to path: fever_data/fever.train.jsonl\n",
        "Data saved! Data size: 24,647"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ZqbkY0lY0h"
      },
      "source": [
        "### Downloading all the files from the repository to the local server.\n",
        "- [ ] Base-Line Data\n",
        "- [ ] Installing The Depenencies \n",
        "- [ ] Training & Evaluation Scripts \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfetCDgLm-8P"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir fever_data \n",
        "!cd fever_data & curl -L https://raw.githubusercontent.com/minwhoo/CrossAug/master/download_data.sh |sh\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/utils_fever.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/run_fever.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/modeling_bert.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/run_fever.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/run_crossaug.py\n",
        "!pip install jsonlines==2.0.0  nltk==3.6.2 numpy==1.20.2 pandas==1.1.5  scikit-learn==0.24.2  scipy==1.6.3  sentencepiece==0.1.95 tensorboardX==2.2  torch==1.8.1  transformers==4.11.2  pytorch-transformers==1.2.0  tqdm==4.60.0\n",
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gnwEPdjLjIW"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjWuEdnWHUvV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Symmetric data\n",
        "run_crossaug.py   --in_file fever_data/symmetric.dev.jsonl   --out_file fever_data/symmetric.train.jsonl\n",
        "# FEVER data\n",
        "run_crossaug.py   --in_file fever_data/fever.dev.jsonl   --out_file fever_data/fever.train.jsonl\n",
        "# FM2 data\n",
        "run_crossaug.py   --in_file fever_data/fm2.dev.jsonl   --out_file fever_data/fm2.train.jsonl\n",
        "# Adversarial data\n",
        "run_crossaug.py   --in_file fever_data/adversarial.dev.jsonl   --out_file fever_data/adversarial.train.jsonl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyLBjcr2L-BN"
      },
      "source": [
        "# Use the fine-tuned negative claim generation model (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOgfqrG6L-y9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = 'minwhoo/bart-base-negative-claim-generation'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "examples = [\n",
        "    \"Little Miss Sunshine was filmed over 30 days.\",\n",
        "    \"Magic Johnson did not play for the Lakers.\",\n",
        "    \"Claire Danes is wedded to an actor from England.\"\n",
        "]\n",
        "\n",
        "batch = tokenizer(examples, max_length=1024, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "out = model.generate(batch['input_ids'].to(model.device), num_beams=5)\n",
        "negative_examples = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "print(negative_examples)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use the fine-tuned negative claim generation model (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import jsonlines\n",
        "from tqdm import trange, tqdm\n",
        "from nltk import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "def find_substitution_map(sent1, sent2):\n",
        "    \"\"\"Find overlapping words in the given two sentences\"\"\"\n",
        "    words1 = word_tokenize(sent1)\n",
        "    words2 = word_tokenize(sent2)\n",
        "    start_idx = 0\n",
        "    while words1[start_idx] == words2[start_idx]:\n",
        "        start_idx += 1\n",
        "        if start_idx == len(words1) or start_idx == len(words2):\n",
        "            return None\n",
        "\n",
        "    end_idx = -1\n",
        "    while words1[end_idx] == words2[end_idx]:\n",
        "        end_idx -= 1\n",
        "\n",
        "    if end_idx == -1:\n",
        "        words_overlap1 = words1[start_idx:]\n",
        "        words_overlap2 = words2[start_idx:]\n",
        "    else:\n",
        "        words_overlap1 = words1[start_idx:end_idx+1]\n",
        "        words_overlap2 = words2[start_idx:end_idx+1]\n",
        "\n",
        "    if 0 < len(words_overlap1) <= 3 and 0 < len(words_overlap2) <= 3:\n",
        "        return words_overlap1, words_overlap2\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def substitute_sent(sent, orig_words, replacing_words):\n",
        "    \"\"\"Find and substitute word phrases from given sentence\"\"\"\n",
        "    sent_words = word_tokenize(sent)\n",
        "    j = 0\n",
        "    match_start_idx = None\n",
        "    match_end_idx = None\n",
        "    matches = []\n",
        "    for i in range(len(sent_words)):\n",
        "        if sent_words[i] == orig_words[j]:\n",
        "            if j == 0:\n",
        "                match_start_idx = i\n",
        "            j += 1\n",
        "        else:\n",
        "            j = 0\n",
        "            match_start_idx = None\n",
        "            match_end_idx = None\n",
        "        if j == len(orig_words):\n",
        "            match_end_idx = i\n",
        "            matches.append((match_start_idx, match_end_idx))\n",
        "            j = 0\n",
        "            match_start_idx = None\n",
        "            match_end_idx = None\n",
        "    if len(matches) == 1:\n",
        "        i, j = matches[0]\n",
        "        return ' '.join(sent_words[:i] + replacing_words + sent_words[j+1:])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_negative_claims(data, batch_size):\n",
        "    \"\"\"Generate negative (refuted) claims using fine-tuned negative claim generation model\"\"\"\n",
        "    model_name = 'minwhoo/bart-base-negative-claim-generation'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for i in trange(0, len(data), batch_size):\n",
        "        sents = [d['claim'] for d in data[i:i+batch_size]]\n",
        "        batch = tokenizer(sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        out = model.generate(batch['input_ids'].to(model.device), num_beams=5)\n",
        "        refuted_sents = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "        for j, refuted in enumerate(refuted_sents):\n",
        "            data[i + j]['claim_refuted'] = refuted\n",
        "    return data\n",
        "\n",
        "\n",
        "def augment_start(in_file=\"file.txt\",out_file=\"file.txt\",batch_size=64 ): \n",
        "\n",
        "    print(f\"Reading from path: {in_file}\")\n",
        "    with jsonlines.open(in_file, mode='r') as reader:\n",
        "        data = [obj for obj in reader]\n",
        "    print(f\"Data loaded! Data size: {len(data):,}\")\n",
        "\n",
        "    print('Generate negative claims')\n",
        "    start_time = time.time()\n",
        "    data = generate_negative_claims(data, batch_size)\n",
        "    print(f\"time took: {time.time() - start_time}\")\n",
        "\n",
        "    print('Modify evidence using lexical search-based substitution')\n",
        "    failed_cnt = 0\n",
        "    start_time = time.time()\n",
        "    for d in tqdm(data):\n",
        "        try:\n",
        "            span_pair = find_substitution_map(d['claim'], d['claim_refuted'])\n",
        "        except:\n",
        "            failed_cnt += 1\n",
        "        else:\n",
        "            if span_pair is not None:\n",
        "                orig_span, replace_span  = span_pair\n",
        "                evid_refuted = substitute_sent(d['evidence'], orig_span, replace_span)\n",
        "                if evid_refuted is not None:\n",
        "                    d['evidence_refuted'] = evid_refuted\n",
        "    print(f\"time took: {time.time() - start_time}\")\n",
        "\n",
        "    print('Augment data')\n",
        "    augmented_data = []\n",
        "    for d in data:\n",
        "        augmented_data.append({\n",
        "            'gold_label': d['gold_label'],\n",
        "            'evidence': d['evidence'],\n",
        "            'claim': d['claim'],\n",
        "            'id': len(augmented_data),\n",
        "            'weight': 0.0,\n",
        "        })\n",
        "        if d['gold_label'] == 'SUPPORTS':\n",
        "            augmented_data.append({\n",
        "                    'gold_label': 'REFUTES',\n",
        "                    'evidence': d['evidence'],\n",
        "                    'claim': d['claim_refuted'],\n",
        "                    'id': len(augmented_data),\n",
        "                    'weight': 0.0,\n",
        "                })\n",
        "            if 'evidence_refuted' in d:\n",
        "                augmented_data.append({\n",
        "                        'gold_label': 'REFUTES',\n",
        "                        'evidence': d['evidence_refuted'],\n",
        "                        'claim': d['claim'],\n",
        "                        'id': len(augmented_data),\n",
        "                        'weight': 0.0,\n",
        "                    })\n",
        "                augmented_data.append({\n",
        "                        'gold_label': 'SUPPORTS',\n",
        "                        'evidence': d['evidence_refuted'],\n",
        "                        'claim': d['claim_refuted'],\n",
        "                        'id': len(augmented_data),\n",
        "                        'weight': 0.0,\n",
        "                    })\n",
        "\n",
        "    print(f\"Saving to path: {out_file}\")\n",
        "    with jsonlines.open(out_file, mode='w') as writer:\n",
        "        writer.write_all(augmented_data)\n",
        "    print(f\"Data saved! Data size: {len(augmented_data):,}\")\n",
        "\n",
        "augment_start(\"fever_data/fever.train.jsonl\",\"fever_data/fever+crossaug.train.jsonl\",64)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using HuggingFace's Model for CrossAug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install autocuda gradio \n",
        "!pip install pyabsa[dev] -U\n",
        "!pip install --upgrade huggingface-hub -U\n",
        "\n",
        "\n",
        "from ast import Str\n",
        "import gradio as gr\n",
        "from tweetnlp import Sentiment, NER\n",
        "from typing import Tuple, Dict\n",
        "from statistics import mean\n",
        "\n",
        "def clean_tweet(tweet: str, remove_chars: str = \"@#\") -> str:\n",
        "    \"\"\"Remove any unwanted characters\n",
        "    Args:\n",
        "        tweet (str): The raw tweet\n",
        "        remove_chars (str, optional): The characters to remove. Defaults to \"@#\".\n",
        "    Returns:\n",
        "        str: The tweet with these characters removed\n",
        "    \"\"\"\n",
        "    for char in remove_chars:\n",
        "        tweet = tweet.replace(char, \"\")\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def format_sentiment(model_output: Dict) -> Dict:\n",
        "    \"\"\"Format the output of the sentiment model\n",
        "    Args:\n",
        "        model_output (Dict): The model output\n",
        "    Returns:\n",
        "        Dict: The format for gradio\n",
        "    \"\"\"\n",
        "    formatted_output = dict()\n",
        "    print(model_output)\n",
        "\n",
        "    try:\n",
        "      if model_output[\"label\"] == \"positive\":\n",
        "          formatted_output[\"positive\"] = model_output[\"probability\"]\n",
        "          formatted_output[\"negative\"] = 1 - model_output[\"probability\"]\n",
        "      else:\n",
        "          formatted_output[\"negative\"] = model_output[\"probability\"]\n",
        "          formatted_output[\"positive\"] = 1 - model_output[\"probability\"]\n",
        "      return formatted_output\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "def format_entities(model_output: Dict) -> Dict:\n",
        "    \"\"\"Format the output of the NER model\n",
        "    Args:\n",
        "        model_output (Dict): The model output\n",
        "    Returns:\n",
        "        Dict: The format for gradio\n",
        "    \"\"\"\n",
        "    formatted_output = dict()\n",
        "    for entity in model_output[\"entity_prediction\"]:\n",
        "        new_output = dict()\n",
        "        name = \" \".join(entity[\"entity\"])\n",
        "        entity_type = entity[\"type\"]\n",
        "        new_key = f\"{name}:{entity_type}\"\n",
        "        new_value = mean(entity[\"probability\"])\n",
        "        formatted_output[new_key] = new_value\n",
        "    return formatted_output\n",
        "\n",
        "\n",
        "def classify(tweet: str) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"Runs models\n",
        "    Args:\n",
        "        tweet (str): The raw tweet\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: The formatted_sentiment and formatted_entities of the tweet\n",
        "    \"\"\"\n",
        "    tweet = clean_tweet(tweet)\n",
        "    # Get sentiment\n",
        "    model_sentiment = se_model.sentiment(tweet)\n",
        "    model_pred = se_model.predict(tweet)\n",
        "    print(model_sentiment)\n",
        "    print(model_pred)\n",
        "    formatted_sentiment = format_sentiment(model_sentiment)\n",
        "    # Get entities\n",
        "    entities = ner_model.ner(tweet)\n",
        "    formatted_entities = format_entities(entities)\n",
        "    return formatted_sentiment, formatted_entities\n",
        "\n",
        "    # https://github.com/cardiffnlp/tweetnlp\n",
        "    \n",
        "\n",
        "def run(tweets=None):\n",
        "  se_model = Sentiment()\n",
        "  ner_model = NER()\n",
        "\n",
        "examples = list()\n",
        "examples.append(\"Dameon Pierce is clearly the #Texans starter and he once again looks good\")\n",
        "examples.append(\"Deebo Samuel had 150+ receiving yards in 4 games last year - the most by any receiver in the league.\")\n",
        "\n",
        "for tweet in examples:\n",
        "  classify(tweet)\n",
        "    # Get a few examples from: https://twitter.com/NFLFantasy\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```javascript Dameon Pierce is clearly the Texans starter and he once again looks good\n",
        "{'label': 'positive'}\n",
        "Dameon Pierce is clearly the Texans starter and he once again looks good\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GhZzzXCN23A"
      },
      "source": [
        "# Generating Evaluation and Training Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL3_AzCAN2MB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve43ekRXN1F-"
      },
      "outputs": [],
      "source": [
        "/anaconda/envs/azureml_py310_sdkv2/bin/python run_fever.py --task_name fever --do_train --train_task_name fever+crossaug  --eval_task_names fever  --data_dir ./fever_data/ --do_lower_case --model_type bert --model_name_or_path bert-base-uncased --max_seq_length 128 --per_gpu_train_batch_size 32  --num_train_epochs 1.0 --save_steps 1000 --output_dir ./out --output_preds --seed 177697310"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIjYl9hyN13q"
      },
      "source": [
        "***italicized text***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVNw3-nxL52S"
      },
      "outputs": [],
      "source": [
        "/anaconda/envs/azureml_py310_sdkv2/bin/python run_fever.py --task_name fever --do_train --train_task_name fever+crossaug --do_eval --eval_task_names fever symmetric adversarial fm2 --data_dir ./fever_data/ --do_lower_case --model_type bert --model_name_or_path bert-base-uncased --max_seq_length 128 --per_gpu_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3.0 --save_steps 100000 --output_dir ./out --output_preds --seed 177697310"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pYSG4d6kJj6",
        "outputId": "82ec9e35-3798-4671-b370-1f193905e8f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Little Miss Sunshine was filmed less than 3 days.', 'Magic Johnson played for the Lakers.', 'Claire Danes is married to an actor from France.']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model_name = 'minwhoo/bart-base-negative-claim-generation'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIu6wI5vePuG",
        "outputId": "da9e68da-786f-4859-9522-8b7508bf4491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "run_fever.py --task_name fever --do_train --train_task_name fever+crossaug --do_eval --eval_task_names fever symmetric adversarial fm2 --data_dir ./fever_data/ --do_lower_case --model_type bert --model_name_or_path bert-base-uncased --max_seq_length 128 --per_gpu_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3.0 --save_steps 100000 --output_dir ./out --output_preds --seed 177697310\n"
          ]
        }
      ],
      "source": [
        "python  run_fever.py \\\n",
        "    --task_name fever \\\n",
        "    --do_train \\\n",
        "    --train_task_name fever+crossaug \\\n",
        "    --do_eval \\\n",
        "    --eval_task_names fever symmetric adversarial fm2 \\\n",
        "    --data_dir ./fever_data/ \\\n",
        "    --do_lower_case \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --save_steps 100000 \\\n",
        "    --output_dir ./out \\\n",
        "    --output_preds \\\n",
        "    --seed 177697310\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "04/22/2023 03:06:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
        "04/22/2023 03:06:23 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
        "04/22/2023 03:06:23 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
        "  \"architectures\": [\n",
        "    \"BertForMaskedLM\"\n",
        "  ],\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"finetuning_task\": \"fever\",\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"layer_norm_eps\": 1e-12,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"model_type\": \"bert\",\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"num_labels\": 3,\n",
        "  \"output_attentions\": false,\n",
        "  \"output_hidden_states\": false,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"pruned_heads\": {},\n",
        "  \"torchscript\": false,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "04/22/2023 03:06:24 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
        "04/22/2023 03:06:25 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
        "04/22/2023 03:06:30 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
        "04/22/2023 03:06:30 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
        "04/22/2023 03:06:33 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='./fever_data/', train_task_name='fever', eval_task_names=['fever', 'symmetric', 'adversarial', 'fm2'], model_type='bert', model_name_or_path='bert-base-uncased', task_name='fever', output_dir='./baseline_trained_models_seed=177697310/', output_preds=True, config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=True, weighted_loss=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=100000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=177697310, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), output_mode='classification')\n",
        "04/22/2023 03:06:33 - INFO - __main__ -   Creating features from dataset file at ./fever_data/\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   Writing example 0 of 242911\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 150448\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] roman at ##wood is a content creator . [SEP] he is best known for his v ##log ##s , where he posts updates about his life on a daily basis . [SEP]\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 3142 2012 3702 2003 1037 4180 8543 1012 102 2002 2003 2190 2124 2005 2010 1058 21197 2015 1010 2073 2002 8466 14409 2055 2010 2166 2006 1037 3679 3978 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 150448\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] roman at ##wood is a content creator . [SEP] he also has another youtube channel called ` ` roman ##at ##wood ' ' , where he posts prank ##s . [SEP]\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 3142 2012 3702 2003 1037 4180 8543 1012 102 2002 2036 2038 2178 7858 3149 2170 1036 1036 3142 4017 3702 1005 1005 1010 2073 2002 8466 26418 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 214861\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] history of art includes architecture , dance , sculpture , music , painting , poetry literature , theatre , narrative , film , photography and graphic arts . [SEP] the subsequent expansion of the list of principal arts in the 20th century reached to nine : architecture , dance , sculpture , music , painting , poetry - l ##rb - described broadly as a form of literature with aesthetic purpose or function , which also includes the distinct genres of theatre and narrative - rr ##b - , film , photography and graphic arts . [SEP]\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 2381 1997 2396 2950 4294 1010 3153 1010 6743 1010 2189 1010 4169 1010 4623 3906 1010 3004 1010 7984 1010 2143 1010 5855 1998 8425 2840 1012 102 1996 4745 4935 1997 1996 2862 1997 4054 2840 1999 1996 3983 2301 2584 2000 3157 1024 4294 1010 3153 1010 6743 1010 2189 1010 4169 1010 4623 1011 1048 15185 1011 2649 13644 2004 1037 2433 1997 3906 2007 12465 3800 2030 3853 1010 2029 2036 2950 1996 5664 11541 1997 3004 1998 7984 1011 25269 2497 1011 1010 2143 1010 5855 1998 8425 2840 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 156709\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] ad ##rien ##ne bail ##on is an accountant . [SEP] ad ##rien ##ne eliza houghton - l ##rb - nee bail ##on ; born october 24 , 1983 - rr ##b - is an american singer - songwriter , recording artist , actress , dancer and television personality . [SEP]\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 4748 23144 2638 15358 2239 2003 2019 17907 1012 102 4748 23144 2638 13234 21234 1011 1048 15185 1011 7663 15358 2239 1025 2141 2255 2484 1010 3172 1011 25269 2497 1011 2003 2019 2137 3220 1011 6009 1010 3405 3063 1010 3883 1010 8033 1998 2547 6180 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   label: REFUTES (id = 1)\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 33078\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] the boston celtics play their home games at td garden . [SEP] the celtics play their home games at the td garden , which they share with the national hockey league - l ##rb - nhl - rr ##b - ' s boston bruins . [SEP]\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 1996 3731 23279 2377 2037 2188 2399 2012 14595 3871 1012 102 1996 23279 2377 2037 2188 2399 2012 1996 14595 3871 1010 2029 2027 3745 2007 1996 2120 3873 2223 1011 1048 15185 1011 7097 1011 25269 2497 1011 1005 1055 3731 18159 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
        "04/22/2023 03:06:43 - INFO - utils_fever -   Writing example 10000 of 242911\n",
        "04/22/2023 03:06:50 - INFO - utils_fever -   Writing example 20000 of 242911\n",
        "04/22/2023 03:06:56 - INFO - utils_fever -   Writing example 30000 of 242911\n",
        "04/22/2023 03:07:05 - INFO - utils_fever -   Writing example 40000 of 242911\n",
        "04/22/2023 03:07:11 - INFO - utils_fever -   Writing example 50000 of 242911\n",
        "04/22/2023 03:07:19 - INFO - utils_fever -   Writing example 60000 of 242911\n",
        "04/22/2023 03:07:27 - INFO - utils_fever -   Writing example 70000 of 242911\n",
        "04/22/2023 03:07:32 - INFO - utils_fever -   Writing example 80000 of 242911\n",
        "04/22/2023 03:07:41 - INFO - utils_fever -   Writing example 90000 of 242911\n",
        "04/22/2023 03:07:47 - INFO - utils_fever -   Writing example 100000 of 242911\n",
        "04/22/2023 03:07:56 - INFO - utils_fever -   Writing example 110000 of 242911\n",
        "04/22/2023 03:08:02 - INFO - utils_fever -   Writing example 120000 of 242911\n",
        "04/22/2023 03:08:08 - INFO - utils_fever -   Writing example 130000 of 242911\n",
        "04/22/2023 03:08:17 - INFO - utils_fever -   Writing example 140000 of 242911\n",
        "04/22/2023 03:08:22 - INFO - utils_fever -   Writing example 150000 of 242911\n",
        "04/22/2023 03:08:32 - INFO - utils_fever -   Writing example 160000 of 242911\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrCGGp3ZDhY5"
      },
      "source": [
        "---\n",
        "# Results\n",
        "\n",
        "Training and evaluation with the above commands should result in the following accuracies.\n",
        "\n",
        "|          | FEVER dev | Symmetric | Adversarial | FM2 dev   |\n",
        "|----------|-----------|-----------|-------------|-----------|\n",
        "| No aug   | **86.43** | 59.14     | 50.00       | 41.15     |\n",
        "| PoE      | 86.14     | 63.88     | 51.31       | **47.39** |\n",
        "| CrossAug | 85.05     | **68.20** | **52.48**   | 45.17     |\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sC-2RGYFDnyS"
      },
      "source": [
        "### Azure Machine Learning Setup (Optional)\n",
        "\n",
        "```javascript\n",
        "because of the size of the data, I have decided to use Azure Machine Learning to run the experiments to make the deadline.\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup & Requirements\n",
        "\n",
        "```javascript\n",
        "The following are the requirements to run the experiments:\n",
        "```\n",
        "- [ ] Azure Subscription\n",
        "- [ ] Azure Machine Learning Workspace\n",
        "- [ ] Azure Machine Learning Compute Cluster or Instance\n",
        "- [ ] Azure Machine Learning Experiment\n",
        "\n",
        "\n",
        "### Azure Machine Learning Workspace\n",
        "\n",
        "```javascript\n",
        "Note: You can create a workspace using the Azure Portal, Azure CLI, or Azure Machine Learning SDK. For more information, see Create an Azure Machine Learning workspace.\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Azure Machine Learning Compute Cluster or Instance\n",
        "\n",
        "```javascript\n",
        "\n",
        "```\n",
        "![image.png](https://raw.githubusercontent.com/MurtadhaM/ML-5156/main/AZURE.png)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Visual Studio Code (VSCode) Integration\n",
        "\n",
        "The following extensions are recommended for VSCode:\n",
        "\n",
        "In the Azure ML portal, click on \"Open in VSCode\" to open the project in VSCode.  This will automatically install the Azure Machine Learning extension for VSCode.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![VSCODE](https://raw.githubusercontent.com/MurtadhaM/ML-5156/main/VSCODE.png)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
