{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bGVcdUrA13_v"
      },
      "source": [
        "# Standard Data Preparation Techniques\n",
        "\n",
        "## Disclaimer:\n",
        "```javascript\n",
        "This is a work in progress. I will be adding more techniques as I learn them. If you have any suggestions, please feel free to reach out to me.\n",
        "```\n",
        "## Credits:\n",
        "```javascript\n",
        "This work is inspired by the following previous work sources:\n",
        "https://github.com/minwhoo/CrossAug\n",
        "\n",
        "author: Minwoo Lee\n",
        "\n",
        "citation: Minwoo Lee, Seungwon Do, and Sung Ju Hwang. 2020. CrossAug: Cross-lingual Data Augmentation for Low-resource Neural Machine Translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020). Association for Computational Linguistics, Online, July 5-10, 2020, pages 1-11. https://www.aclweb.org/anthology/2020.acl-main.1\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "- [ ] Data Cleaning: Identifying and correcting mistakes or errors in the data.\n",
        "- [ ] Feature Selection: Identifying those input variables that are most relevant to the task.\n",
        "- [ ] Data Transforms: Changing the scale or distribution of variables.\n",
        "- [ ] Dimensionality Reduction: Creating compact projections of the data.\n",
        "- [ ] Feature Engineering: Deriving new variables from available data.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Note:\n",
        "```javascript\n",
        "In Text Processing, we Tokenizer,  meaning convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors. Other types of Data transformation is needed for different tasks lie Feature extractor (Audio), ImageProcessor (Images), or Processor (Multimodal).\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfetCDgLm-8P"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir fever_data \n",
        "!cd fever_data & curl -L https://raw.githubusercontent.com/minwhoo/CrossAug/master/download_data.sh |sh\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/utils_fever.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/run_fever.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/modeling_bert.py\n",
        "!curl -O https://raw.githubusercontent.com/minwhoo/CrossAug/master/run_fever.py\n",
        "!pip install jsonlines==2.0.0  nltk==3.6.2 numpy==1.20.2 pandas==1.1.5  scikit-learn==0.24.2  scipy==1.6.3  sentencepiece==0.1.95 tensorboardX==2.2  torch==1.8.1  transformers==4.11.2  pytorch-transformers==1.2.0  tqdm==4.60.0\n",
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1TL3kSHXBgt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = 'minwhoo/bart-base-negative-claim-generation'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pYSG4d6kJj6",
        "outputId": "82ec9e35-3798-4671-b370-1f193905e8f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Little Miss Sunshine was filmed less than 3 days.', 'Magic Johnson played for the Lakers.', 'Claire Danes is married to an actor from France.']\n"
          ]
        }
      ],
      "source": [
        "examples = [\n",
        "    \"Little Miss Sunshine was filmed over 30 days.\",\n",
        "    \"Magic Johnson did not play for the Lakers.\",\n",
        "    \"Claire Danes is wedded to an actor from England.\"\n",
        "]\n",
        "\n",
        "batch = tokenizer(examples, max_length=1024, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "out = model.generate(batch['input_ids'].to(model.device), num_beams=5)\n",
        "negative_examples = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "print(negative_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14JbjqWplhie"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import jsonlines\n",
        "from tqdm import trange, tqdm\n",
        "from nltk import word_tokenize\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "def find_substitution_map(sent1, sent2):\n",
        "    \"\"\"Find overlapping words in the given two sentences\"\"\"\n",
        "    words1 = word_tokenize(sent1)\n",
        "    words2 = word_tokenize(sent2)\n",
        "    start_idx = 0\n",
        "    while words1[start_idx] == words2[start_idx]:\n",
        "        start_idx += 1\n",
        "        if start_idx == len(words1) or start_idx == len(words2):\n",
        "            return None\n",
        "\n",
        "    end_idx = -1\n",
        "    while words1[end_idx] == words2[end_idx]:\n",
        "        end_idx -= 1\n",
        "\n",
        "    if end_idx == -1:\n",
        "        words_overlap1 = words1[start_idx:]\n",
        "        words_overlap2 = words2[start_idx:]\n",
        "    else:\n",
        "        words_overlap1 = words1[start_idx:end_idx+1]\n",
        "        words_overlap2 = words2[start_idx:end_idx+1]\n",
        "\n",
        "    if 0 < len(words_overlap1) <= 3 and 0 < len(words_overlap2) <= 3:\n",
        "        return words_overlap1, words_overlap2\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def substitute_sent(sent, orig_words, replacing_words):\n",
        "    \"\"\"Find and substitute word phrases from given sentence\"\"\"\n",
        "    sent_words = word_tokenize(sent)\n",
        "    j = 0\n",
        "    match_start_idx = None\n",
        "    match_end_idx = None\n",
        "    matches = []\n",
        "    for i in range(len(sent_words)):\n",
        "        if sent_words[i] == orig_words[j]:\n",
        "            if j == 0:\n",
        "                match_start_idx = i\n",
        "            j += 1\n",
        "        else:\n",
        "            j = 0\n",
        "            match_start_idx = None\n",
        "            match_end_idx = None\n",
        "        if j == len(orig_words):\n",
        "            match_end_idx = i\n",
        "            matches.append((match_start_idx, match_end_idx))\n",
        "            j = 0\n",
        "            match_start_idx = None\n",
        "            match_end_idx = None\n",
        "    if len(matches) == 1:\n",
        "        i, j = matches[0]\n",
        "        return ' '.join(sent_words[:i] + replacing_words + sent_words[j+1:])\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_negative_claims(data, batch_size):\n",
        "    \"\"\"Generate negative (refuted) claims using fine-tuned negative claim generation model\"\"\"\n",
        "    model_name = 'minwhoo/bart-base-negative-claim-generation'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for i in trange(0, len(data), batch_size):\n",
        "        sents = [d['claim'] for d in data[i:i+batch_size]]\n",
        "        batch = tokenizer(sents, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        out = model.generate(batch['input_ids'].to(model.device), num_beams=5)\n",
        "        refuted_sents = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "        for j, refuted in enumerate(refuted_sents):\n",
        "            data[i + j]['claim_refuted'] = refuted\n",
        "    return data\n",
        "\n",
        "\n",
        "def augment_start(in_file=\"file.txt\",out_file=\"file.txt\",batch_size=64 ): \n",
        "\n",
        "    print(f\"Reading from path: {in_file}\")\n",
        "    with jsonlines.open(in_file, mode='r') as reader:\n",
        "        data = [obj for obj in reader]\n",
        "    print(f\"Data loaded! Data size: {len(data):,}\")\n",
        "\n",
        "    print('Generate negative claims')\n",
        "    start_time = time.time()\n",
        "    data = generate_negative_claims(data, batch_size)\n",
        "    print(f\"time took: {time.time() - start_time}\")\n",
        "\n",
        "    print('Modify evidence using lexical search-based substitution')\n",
        "    failed_cnt = 0\n",
        "    start_time = time.time()\n",
        "    for d in tqdm(data):\n",
        "        try:\n",
        "            span_pair = find_substitution_map(d['claim'], d['claim_refuted'])\n",
        "        except:\n",
        "            failed_cnt += 1\n",
        "        else:\n",
        "            if span_pair is not None:\n",
        "                orig_span, replace_span  = span_pair\n",
        "                evid_refuted = substitute_sent(d['evidence'], orig_span, replace_span)\n",
        "                if evid_refuted is not None:\n",
        "                    d['evidence_refuted'] = evid_refuted\n",
        "    print(f\"time took: {time.time() - start_time}\")\n",
        "\n",
        "    print('Augment data')\n",
        "    augmented_data = []\n",
        "    for d in data:\n",
        "        augmented_data.append({\n",
        "            'gold_label': d['gold_label'],\n",
        "            'evidence': d['evidence'],\n",
        "            'claim': d['claim'],\n",
        "            'id': len(augmented_data),\n",
        "            'weight': 0.0,\n",
        "        })\n",
        "        if d['gold_label'] == 'SUPPORTS':\n",
        "            augmented_data.append({\n",
        "                    'gold_label': 'REFUTES',\n",
        "                    'evidence': d['evidence'],\n",
        "                    'claim': d['claim_refuted'],\n",
        "                    'id': len(augmented_data),\n",
        "                    'weight': 0.0,\n",
        "                })\n",
        "            if 'evidence_refuted' in d:\n",
        "                augmented_data.append({\n",
        "                        'gold_label': 'REFUTES',\n",
        "                        'evidence': d['evidence_refuted'],\n",
        "                        'claim': d['claim'],\n",
        "                        'id': len(augmented_data),\n",
        "                        'weight': 0.0,\n",
        "                    })\n",
        "                augmented_data.append({\n",
        "                        'gold_label': 'SUPPORTS',\n",
        "                        'evidence': d['evidence_refuted'],\n",
        "                        'claim': d['claim_refuted'],\n",
        "                        'id': len(augmented_data),\n",
        "                        'weight': 0.0,\n",
        "                    })\n",
        "\n",
        "    print(f\"Saving to path: {out_file}\")\n",
        "    with jsonlines.open(out_file, mode='w') as writer:\n",
        "        writer.write_all(augmented_data)\n",
        "    print(f\"Data saved! Data size: {len(augmented_data):,}\")\n",
        "\n",
        "augment_start(\"fever_data/fever.train.jsonl\",\"fever_data/fever+crossaug.train.jsonl\",64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "Xbf1aU_MsXjF"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Finetuning the library models for sequence classification on GLUE (Bert, XLM, XLNet, RoBERTa).\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertTokenizer,\n",
        "                                  RobertaConfig,\n",
        "                                  RobertaForSequenceClassification,\n",
        "                                  RobertaTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification,\n",
        "                                  XLMTokenizer, XLNetConfig,\n",
        "                                  XLNetForSequenceClassification,\n",
        "                                  XLNetTokenizer)\n",
        "from modeling_bert import BertConfig, BertForSequenceClassification\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIu6wI5vePuG",
        "outputId": "9d017a67-45d9-464d-d9a6-3c23731dda09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "04/22/2023 03:02:55 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/22/2023 03:02:56 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/22/2023 03:02:56 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"fever\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/22/2023 03:02:57 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/22/2023 03:02:58 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "04/22/2023 03:03:04 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/22/2023 03:03:04 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/22/2023 03:03:07 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='./fever_data/', train_task_name='fever+crossaug', eval_task_names=['fever', 'symmetric', 'adversarial', 'fm2'], model_type='bert', model_name_or_path='bert-base-uncased', task_name='fever', output_dir='./crossaug_trained_models_seed=177697310/', output_preds=True, config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=True, weighted_loss=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=100000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=177697310, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), output_mode='classification')\n",
            "04/22/2023 03:03:07 - INFO - __main__ -   Creating features from dataset file at ./fever_data/\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jsonlines/jsonlines.py\", line 162, in read\n",
            "    value = self._loads(line)\n",
            "  File \"/usr/lib/python3.9/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.9/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.9/json/decoder.py\", line 353, in raw_decode\n",
            "    obj, end = self.scan_once(s, idx)\n",
            "json.decoder.JSONDecodeError: Expecting ',' delimiter: line 1 column 221 (char 220)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_fever.py\", line 529, in <module>\n",
            "    main()\n",
            "  File \"/content/run_fever.py\", line 481, in main\n",
            "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
            "  File \"/content/run_fever.py\", line 294, in load_and_cache_examples\n",
            "    examples = processor.get_dev_examples(args.data_dir, task) if evaluate else processor.get_train_examples(args.data_dir, task)\n",
            "  File \"/content/utils_fever.py\", line 409, in get_train_examples\n",
            "    self._read_jsonlines(os.path.join(data_dir, f\"{task}.train.jsonl\")), \"train\")\n",
            "  File \"/content/utils_fever.py\", line 401, in _read_jsonlines\n",
            "    for line in reader.iter(type=dict):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jsonlines/jsonlines.py\", line 203, in iter\n",
            "    yield self.read(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/jsonlines/jsonlines.py\", line 167, in read\n",
            "    raise exc from orig_exc\n",
            "jsonlines.jsonlines.InvalidLineError: line contains invalid json: Expecting ',' delimiter: line 1 column 221 (char 220) (line 3532)\n"
          ]
        }
      ],
      "source": [
        "!python3.9 run_fever.py \\\n",
        "    --task_name fever \\\n",
        "    --do_train \\\n",
        "    --train_task_name fever+crossaug \\\n",
        "    --do_eval \\\n",
        "    --eval_task_names fever symmetric adversarial fm2 \\\n",
        "    --data_dir ./fever_data/ \\\n",
        "    --do_lower_case \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --save_steps 100000 \\\n",
        "    --output_dir ./crossaug_trained_models_seed=177697310/ \\\n",
        "    --output_preds \\\n",
        "    --seed 177697310\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "0eiQATPp1i-J",
        "outputId": "712288f8-40ee-4533-add6-01f2a869268c"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-bf0a93e98ae8>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\" BERT classification fine-tuning: utilities to work with GLUE tasks \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from .utils._tags import (\n\u001b[1;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .validation import (as_float_array,\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[0;31m# TODO: remove in 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                     rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/stats/_continuous_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ufuncs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazyselect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_rvs_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrvs_ratio_uniforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m from ._tukeylambda_stats import (tukeylambda_variance as _tlvar,\n",
            "\u001b[0;32m_stats.pyx\u001b[0m in \u001b[0;36minit scipy.stats._stats\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mcython_special.pyx\u001b[0m in \u001b[0;36minit scipy.special.cython_special\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: scipy.special._ufuncs_cxx does not export expected C variable _export_faddeeva_erfc",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" BERT classification fine-tuning: utilities to work with GLUE tasks \"\"\"\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertTokenizer,\n",
        "                                  RobertaConfig,\n",
        "                                  RobertaForSequenceClassification,\n",
        "                                  RobertaTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification,\n",
        "                                  XLMTokenizer, XLNetConfig,\n",
        "                                  XLNetForSequenceClassification,\n",
        "                                  XLNetTokenizer)\n",
        "from modeling_bert import BertConfig, BertForSequenceClassification\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "from utils_fever import (compute_metrics, convert_examples_to_features,\n",
        "                        output_modes, processors)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, XLNetConfig, XLMConfig, RobertaConfig)), ())\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
        "}\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertTokenizer,\n",
        "                                  RobertaConfig,\n",
        "                                  RobertaForSequenceClassification,\n",
        "                                  RobertaTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification,\n",
        "                                  XLMTokenizer, XLNetConfig,\n",
        "                                  XLNetForSequenceClassification,\n",
        "                                  XLNetTokenizer)\n",
        "from modeling_bert import BertConfig, BertForSequenceClassification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAnfRveY2nM2",
        "outputId": "14b82646-1435-4ad7-954e-e19f24ea6d7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)"
            ]
          },
          "execution_count": 186,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "argparse.ArgumentParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KYlYIolmaES",
        "outputId": "c434c431-48ba-40db-9e49-b27316768d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "04/22/2023 03:06:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/22/2023 03:06:23 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "04/22/2023 03:06:23 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"fever\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 3,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "04/22/2023 03:06:24 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "04/22/2023 03:06:25 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "04/22/2023 03:06:30 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "04/22/2023 03:06:30 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "04/22/2023 03:06:33 - INFO - __main__ -   Training/evaluation parameters Namespace(data_dir='./fever_data/', train_task_name='fever', eval_task_names=['fever', 'symmetric', 'adversarial', 'fm2'], model_type='bert', model_name_or_path='bert-base-uncased', task_name='fever', output_dir='./baseline_trained_models_seed=177697310/', output_preds=True, config_name='', tokenizer_name='', cache_dir='', max_seq_length=128, do_train=True, do_eval=True, evaluate_during_training=False, do_lower_case=True, weighted_loss=False, per_gpu_train_batch_size=32, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=2e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_steps=50, save_steps=100000, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=False, overwrite_cache=False, seed=177697310, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), output_mode='classification')\n",
            "04/22/2023 03:06:33 - INFO - __main__ -   Creating features from dataset file at ./fever_data/\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   Writing example 0 of 242911\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 150448\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] roman at ##wood is a content creator . [SEP] he is best known for his v ##log ##s , where he posts updates about his life on a daily basis . [SEP]\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 3142 2012 3702 2003 1037 4180 8543 1012 102 2002 2003 2190 2124 2005 2010 1058 21197 2015 1010 2073 2002 8466 14409 2055 2010 2166 2006 1037 3679 3978 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 150448\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] roman at ##wood is a content creator . [SEP] he also has another youtube channel called ` ` roman ##at ##wood ' ' , where he posts prank ##s . [SEP]\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 3142 2012 3702 2003 1037 4180 8543 1012 102 2002 2036 2038 2178 7858 3149 2170 1036 1036 3142 4017 3702 1005 1005 1010 2073 2002 8466 26418 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 214861\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] history of art includes architecture , dance , sculpture , music , painting , poetry literature , theatre , narrative , film , photography and graphic arts . [SEP] the subsequent expansion of the list of principal arts in the 20th century reached to nine : architecture , dance , sculpture , music , painting , poetry - l ##rb - described broadly as a form of literature with aesthetic purpose or function , which also includes the distinct genres of theatre and narrative - rr ##b - , film , photography and graphic arts . [SEP]\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 2381 1997 2396 2950 4294 1010 3153 1010 6743 1010 2189 1010 4169 1010 4623 3906 1010 3004 1010 7984 1010 2143 1010 5855 1998 8425 2840 1012 102 1996 4745 4935 1997 1996 2862 1997 4054 2840 1999 1996 3983 2301 2584 2000 3157 1024 4294 1010 3153 1010 6743 1010 2189 1010 4169 1010 4623 1011 1048 15185 1011 2649 13644 2004 1037 2433 1997 3906 2007 12465 3800 2030 3853 1010 2029 2036 2950 1996 5664 11541 1997 3004 1998 7984 1011 25269 2497 1011 1010 2143 1010 5855 1998 8425 2840 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 156709\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] ad ##rien ##ne bail ##on is an accountant . [SEP] ad ##rien ##ne eliza houghton - l ##rb - nee bail ##on ; born october 24 , 1983 - rr ##b - is an american singer - songwriter , recording artist , actress , dancer and television personality . [SEP]\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 4748 23144 2638 15358 2239 2003 2019 17907 1012 102 4748 23144 2638 13234 21234 1011 1048 15185 1011 7663 15358 2239 1025 2141 2255 2484 1010 3172 1011 25269 2497 1011 2003 2019 2137 3220 1011 6009 1010 3405 3063 1010 3883 1010 8033 1998 2547 6180 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   label: REFUTES (id = 1)\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   *** Example ***\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   guid: 33078\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   tokens: [CLS] the boston celtics play their home games at td garden . [SEP] the celtics play their home games at the td garden , which they share with the national hockey league - l ##rb - nhl - rr ##b - ' s boston bruins . [SEP]\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_ids: 101 1996 3731 23279 2377 2037 2188 2399 2012 14595 3871 1012 102 1996 23279 2377 2037 2188 2399 2012 1996 14595 3871 1010 2029 2027 3745 2007 1996 2120 3873 2223 1011 1048 15185 1011 7097 1011 25269 2497 1011 1005 1055 3731 18159 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/22/2023 03:06:35 - INFO - utils_fever -   label: SUPPORTS (id = 0)\n",
            "04/22/2023 03:06:43 - INFO - utils_fever -   Writing example 10000 of 242911\n",
            "04/22/2023 03:06:50 - INFO - utils_fever -   Writing example 20000 of 242911\n",
            "04/22/2023 03:06:56 - INFO - utils_fever -   Writing example 30000 of 242911\n",
            "04/22/2023 03:07:05 - INFO - utils_fever -   Writing example 40000 of 242911\n",
            "04/22/2023 03:07:11 - INFO - utils_fever -   Writing example 50000 of 242911\n",
            "04/22/2023 03:07:19 - INFO - utils_fever -   Writing example 60000 of 242911\n",
            "04/22/2023 03:07:27 - INFO - utils_fever -   Writing example 70000 of 242911\n",
            "04/22/2023 03:07:32 - INFO - utils_fever -   Writing example 80000 of 242911\n",
            "04/22/2023 03:07:41 - INFO - utils_fever -   Writing example 90000 of 242911\n",
            "04/22/2023 03:07:47 - INFO - utils_fever -   Writing example 100000 of 242911\n",
            "04/22/2023 03:07:56 - INFO - utils_fever -   Writing example 110000 of 242911\n",
            "04/22/2023 03:08:02 - INFO - utils_fever -   Writing example 120000 of 242911\n",
            "04/22/2023 03:08:08 - INFO - utils_fever -   Writing example 130000 of 242911\n",
            "04/22/2023 03:08:17 - INFO - utils_fever -   Writing example 140000 of 242911\n",
            "04/22/2023 03:08:22 - INFO - utils_fever -   Writing example 150000 of 242911\n",
            "04/22/2023 03:08:32 - INFO - utils_fever -   Writing example 160000 of 242911\n",
            "04/22/2023 03:08:38 - INFO - utils_fever -   Writing example 170000 of 242911\n"
          ]
        }
      ],
      "source": [
        "!python run_fever.py \\\n",
        "    --task_name fever \\\n",
        "    --do_train \\\n",
        "    --train_task_name fever \\\n",
        "    --do_eval \\\n",
        "    --eval_task_names fever symmetric adversarial fm2 \\\n",
        "    --data_dir ./fever_data/ \\\n",
        "    --do_lower_case \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_train_batch_size 32 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --save_steps 100000 \\\n",
        "    --output_dir ./baseline_trained_models_seed=177697310/ \\\n",
        "    --output_preds \\\n",
        "    --seed 177697310"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2KNrqkY3AHs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV39lHA9G3k2"
      },
      "outputs": [],
      "source": [
        "!pip install autocuda gradio \n",
        "\n",
        "!pip install pyabsa[dev] -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOC5YpqdOrvj"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface-hub -U\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1Rj-ORQANIbL",
        "outputId": "33c1a097-ce9d-4d80-94a8-78cae46b6267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dameon Pierce is clearly the Texans starter and he once again looks good\n",
            "{'label': 'positive'}\n",
            "Dameon Pierce is clearly the Texans starter and he once again looks good\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c5b697cb4749>\u001b[0m in \u001b[0;36m<cell line: 90>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m   \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Get a few examples from: https://twitter.com/NFLFantasy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c5b697cb4749>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Get entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mner_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mformatted_entities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatted_entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c5b697cb4749>\u001b[0m in \u001b[0;36mformat_entities\u001b[0;34m(model_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0mformatted_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entity_prediction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mnew_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "from ast import Str\n",
        "import gradio as gr\n",
        "from tweetnlp import Sentiment, NER\n",
        "from typing import Tuple, Dict\n",
        "from statistics import mean\n",
        "\n",
        "def clean_tweet(tweet: str, remove_chars: str = \"@#\") -> str:\n",
        "    \"\"\"Remove any unwanted characters\n",
        "    Args:\n",
        "        tweet (str): The raw tweet\n",
        "        remove_chars (str, optional): The characters to remove. Defaults to \"@#\".\n",
        "    Returns:\n",
        "        str: The tweet with these characters removed\n",
        "    \"\"\"\n",
        "    for char in remove_chars:\n",
        "        tweet = tweet.replace(char, \"\")\n",
        "    return tweet\n",
        "\n",
        "\n",
        "def format_sentiment(model_output: Dict) -> Dict:\n",
        "    \"\"\"Format the output of the sentiment model\n",
        "    Args:\n",
        "        model_output (Dict): The model output\n",
        "    Returns:\n",
        "        Dict: The format for gradio\n",
        "    \"\"\"\n",
        "    formatted_output = dict()\n",
        "    print(model_output)\n",
        "\n",
        "    try:\n",
        "      if model_output[\"label\"] == \"positive\":\n",
        "          formatted_output[\"positive\"] = model_output[\"probability\"]\n",
        "          formatted_output[\"negative\"] = 1 - model_output[\"probability\"]\n",
        "      else:\n",
        "          formatted_output[\"negative\"] = model_output[\"probability\"]\n",
        "          formatted_output[\"positive\"] = 1 - model_output[\"probability\"]\n",
        "      return formatted_output\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "\n",
        "def format_entities(model_output: Dict) -> Dict:\n",
        "    \"\"\"Format the output of the NER model\n",
        "    Args:\n",
        "        model_output (Dict): The model output\n",
        "    Returns:\n",
        "        Dict: The format for gradio\n",
        "    \"\"\"\n",
        "    formatted_output = dict()\n",
        "    for entity in model_output[\"entity_prediction\"]:\n",
        "        new_output = dict()\n",
        "        name = \" \".join(entity[\"entity\"])\n",
        "        entity_type = entity[\"type\"]\n",
        "        new_key = f\"{name}:{entity_type}\"\n",
        "        new_value = mean(entity[\"probability\"])\n",
        "        formatted_output[new_key] = new_value\n",
        "    return formatted_output\n",
        "\n",
        "\n",
        "def classify(tweet: str) -> Tuple[Dict, Dict]:\n",
        "    \"\"\"Runs models\n",
        "    Args:\n",
        "        tweet (str): The raw tweet\n",
        "    Returns:\n",
        "        Tuple[Dict, Dict]: The formatted_sentiment and formatted_entities of the tweet\n",
        "    \"\"\"\n",
        "    tweet = clean_tweet(tweet)\n",
        "    # Get sentiment\n",
        "    model_sentiment = se_model.sentiment(tweet)\n",
        "    model_pred = se_model.predict(tweet)\n",
        "    print(model_sentiment)\n",
        "    print(model_pred)\n",
        "    formatted_sentiment = format_sentiment(model_sentiment)\n",
        "    # Get entities\n",
        "    entities = ner_model.ner(tweet)\n",
        "    formatted_entities = format_entities(entities)\n",
        "    return formatted_sentiment, formatted_entities\n",
        "\n",
        "    # https://github.com/cardiffnlp/tweetnlp\n",
        "    \n",
        "\n",
        "def run(tweets=None):\n",
        "  se_model = Sentiment()\n",
        "  ner_model = NER()\n",
        "\n",
        "examples = list()\n",
        "examples.append(\"Dameon Pierce is clearly the #Texans starter and he once again looks good\")\n",
        "examples.append(\"Deebo Samuel had 150+ receiving yards in 4 games last year - the most by any receiver in the league.\")\n",
        "\n",
        "for tweet in examples:\n",
        "  classify(tweet)\n",
        "    # Get a few examples from: https://twitter.com/NFLFantasy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncTCfa8vGq5H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_sentiment():\n",
        "  result = pd.DataFrame(\n",
        "        {\n",
        "            \"aspect\": result[\"aspect\"],\n",
        "            \"sentiment\": result[\"sentiment\"],\n",
        "            # 'probability': result[0]['probs'],\n",
        "            \"confidence\": [round(x, 4) for x in result[\"confidence\"]],\n",
        "            \"position\": result[\"position\"],\n",
        "        })\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nNjUtXbzDZS1",
        "outputId": "a05dd14e-1085-4689-bd17-cb4f3e4befe9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NEUTRAL'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred= {'label': 'neutral', 'score': 0.7174134254455566}\n",
        "pred['label'].upper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCnZItg0Xrp2"
      },
      "outputs": [],
      "source": [
        "# Murtadha Marzouq's Sentiment Analysis Project -Replication of https://doi.org/10.1016/j.ijhm.2016.02.001\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from textblob import TextBlob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQXN6n99XYIk"
      },
      "source": [
        "#DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjNKN3N-Wnhy"
      },
      "outputs": [],
      "source": [
        "# Simple Text Cleaning \n",
        "# In the punctuation value to understand what is going on you will have to read up on regex commands. This is getting rid of\n",
        "# punctuation, special charatcters and emojis contained in the tweets and in the text = re.sub is doing a similar task but\n",
        "# taking the extra step to clean the data.\n",
        "# Cleaning the tweets Step 2\n",
        "def clean_tweets_tb(input):\n",
        "    punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
        "    text = str(input)\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(\"@[A-Za-z0-9]+\", \"\", text)\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text)\n",
        "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
        "    text = re.sub(r\"_[A-Za-z0-9]+\", \"\", text)\n",
        "    text = re.sub(r\"__\", \"\", text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = re.sub('[' + punctuation + ']+', ' ', text)  # strip punctuation\n",
        "    text = re.sub('\\s+', ' ', text)  # remove double spacing\n",
        "    text = re.sub('’', ' ', text)  # remove apostrophe'\n",
        "    text = re.sub('\\'', ' ', text)  # remove double spacing\n",
        "    text = re.sub('([0-9]+)', '', text)  # remove numbers\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = text.lower()  # Lower text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q1LnNkbXTk2"
      },
      "source": [
        "# WORD COUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KQ8AdWSlXRu3",
        "outputId": "fbc2e73a-2d48-499e-aa26-93b8c7f05d6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `'stopwords''\n",
            "/bin/bash: -c: line 0: `nltk.download('stopwords')'\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-fdf6bd8ca6bf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtop_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mRE_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'\\b(?:{})\\b'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "!nltk.download('stopwords')\n",
        "top_N = 10\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "RE_stopwords = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
        "# replace '|'-->' ' and drop all stopwords\n",
        "words = (data_clean.tweet\n",
        "           .str.lower()\n",
        "           .replace([r'\\|', RE_stopwords], [' ', ''], regex=True)\n",
        "           .str.cat(sep=' ')\n",
        "           .split()\n",
        ")\n",
        "# generate DF out of Counter\n",
        "rslt = pd.DataFrame(Counter(words).most_common(top_N))\n",
        "print(rslt)\n",
        "# plot\n",
        "\n",
        "\n",
        "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42).generate(str(rslt))\n",
        "fig = plt.figure(\n",
        "    figsize = (40, 30),\n",
        "    facecolor = 'k',\n",
        "    edgecolor = 'k')\n",
        "plt.subplot(3, 4, index+1)\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtMn_s46XnKE"
      },
      "source": [
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3WfNaajXriQ"
      },
      "outputs": [],
      "source": [
        "data_sentiment = data_clean.copy()\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "data_sentiment['polarity'] = data_sentiment['tweet'].apply(pol)\n",
        "data_sentiment['subjectivity'] = data_sentiment['tweet'].apply(sub)\n",
        "data_sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq7tlcF-Xyzr"
      },
      "source": [
        "# Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5wBTIKCX2FR"
      },
      "outputs": [],
      "source": [
        "print('Average polarity for quarantine related tweets = ',data_sentiment['polarity'].mean())\n",
        "print('Average subjectivity for quarantine related tweets = ',data_sentiment['subjectivity'].mean())\n",
        "print(data_sentiment.isin([0]).sum()) \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
